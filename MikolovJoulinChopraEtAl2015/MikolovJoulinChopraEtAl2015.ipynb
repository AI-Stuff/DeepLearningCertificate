{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Structurally Contrained Recurrent Network (SCRN) Model\n",
    "#\n",
    "# This gives an implementation of the SCRN model given in Mikolov et al. 2015, arXiv:1412.7753 [cs.NE], \n",
    "# https://arxiv.org/abs/1412.7753 using Python and Tensorflow.\n",
    "#\n",
    "# This IPython Notebook provides an example of how to call the associated library of Python scripts.  \n",
    "# Mikolov et al. should be consulted to make sure of the correct hyperparameter values.\n",
    "#\n",
    "# Stuart Hagler, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# Local Imports\n",
    "sys.path.insert(0, 'python')\n",
    "from lstm import lstm_graph\n",
    "from read_data import read_data\n",
    "from scrn import scrn_graph\n",
    "from srn import srn_graph\n",
    "from tokens import text_elements_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flags\n",
    "rnn_flg = 2      # 1 for SRN\n",
    "                 # 2 for LSTM\n",
    "                 # 3 for SCRN\n",
    "usecase_flg = 1  # 1 for predicting letters\n",
    "                 # 2 for predicting words with cutoff for infrequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network-specific hyperparameters\n",
    "if rnn_flg == 1:\n",
    "    \n",
    "    # Network hyperparameters\n",
    "    hidden_size = 110         # Dimension of the hidden vector\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_unfoldings = 10       # Total number of unfoldings\n",
    "    \n",
    "elif rnn_flg == 2:\n",
    "    \n",
    "    # Network hyperparameters\n",
    "    hidden_size = 110         # Dimension of the hidden vector\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_unfoldings = 10       # Total number of unfoldings\n",
    "    \n",
    "elif rnn_flg == 3:\n",
    "    \n",
    "    # Network hyperparameters\n",
    "    alpha = 0.95\n",
    "    hidden_size = 100         # Dimension of the hidden vector\n",
    "    state_size = 10           # Dimension of the state vector\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_unfoldings = 50       # Total number of unfoldings\n",
    "    \n",
    "# General network hyperparameters\n",
    "word_frequency_cutoff = 50    # Cutoff for infrequent words for usecase_flg = 2\n",
    "\n",
    "# General training hyperparameters\n",
    "batch_size = 32               # Batch size for each tower\n",
    "learning_decay = 1/1.5        # Multiplier to decay the learn rate when required\n",
    "learning_rate = 0.05          # Initial learning rate\n",
    "num_epochs = 100              # Total number of epochs to run the algorithm\n",
    "num_gpus = 1                  # Number of GPUs (towers) available\n",
    "optimization_frequency = 5    # Number of unfoldings before optimization step\n",
    "summary_frequency = 500       # Summary information is displayed after training this many batches\n",
    "\n",
    "# Data file\n",
    "filename = 'data/text8.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare training and validation batches\n",
    "raw_data = read_data(usecase_flg, filename)\n",
    "data, dictionary, reverse_dictionary, vocabulary_size = text_elements_to_tokens(usecase_flg, raw_data, \n",
    "                                                                                word_frequency_cutoff)\n",
    "training_size = math.floor((10/11)*len(raw_data)/num_gpus)\n",
    "validation_size = math.floor((1/11)*len(raw_data)/num_gpus)\n",
    "training_text = []\n",
    "validation_text = []\n",
    "for i in range(num_gpus):\n",
    "    training_text.append(data[i*training_size:(i+1)*training_size])\n",
    "    validation_text.append(data[num_gpus*training_size + i*validation_size: \\\n",
    "                                num_gpus*training_size + (i+1)*validation_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 28\n",
      "Training Batch Generator:\n",
      "     Tower: 0\n",
      "          Input Text Size: 9090909\n",
      "          Cut Text Size: 9090880\n",
      "          Subtext Size: 284090\n",
      "          Dropped Text Size: 29\n",
      "          Effective Batch Size: 320\n",
      "          Number of Batches: 28409\n",
      "Validation Batch Generator:\n",
      "     Tower: 0\n",
      "          Input Text Size: 9090909\n",
      "          Cut Text Size: 9090909\n",
      "          Subtext Size: 9090909\n",
      "          Dropped Text Size: 0\n",
      "          Effective Batch Size: 1\n",
      "          Number of Batches: 9090909\n",
      "Initialized\n",
      "Epoch: 1  Learning Rate: 0.05\n",
      "     Total Batches: 500  Current Batch: 500  Cost: 2.88\n",
      "     Total Batches: 1000  Current Batch: 1000  Cost: 2.83\n",
      "     Total Batches: 1500  Current Batch: 1500  Cost: 2.91\n",
      "     Total Batches: 2000  Current Batch: 2000  Cost: 2.85\n",
      "     Total Batches: 2500  Current Batch: 2500  Cost: 2.78\n",
      "     Total Batches: 3000  Current Batch: 3000  Cost: 2.77\n",
      "     Total Batches: 3500  Current Batch: 3500  Cost: 2.80\n",
      "     Total Batches: 4000  Current Batch: 4000  Cost: 2.79\n",
      "     Total Batches: 4500  Current Batch: 4500  Cost: 2.62\n",
      "     Total Batches: 5000  Current Batch: 5000  Cost: 2.62\n",
      "     Total Batches: 5500  Current Batch: 5500  Cost: 2.64\n",
      "     Total Batches: 6000  Current Batch: 6000  Cost: 2.70\n",
      "     Total Batches: 6500  Current Batch: 6500  Cost: 2.52\n",
      "     Total Batches: 7000  Current Batch: 7000  Cost: 2.58\n",
      "     Total Batches: 7500  Current Batch: 7500  Cost: 2.56\n",
      "     Total Batches: 8000  Current Batch: 8000  Cost: 2.55\n",
      "     Total Batches: 8500  Current Batch: 8500  Cost: 2.75\n",
      "     Total Batches: 9000  Current Batch: 9000  Cost: 2.53\n",
      "     Total Batches: 9500  Current Batch: 9500  Cost: 2.61\n",
      "     Total Batches: 10000  Current Batch: 10000  Cost: 2.68\n",
      "     Total Batches: 10500  Current Batch: 10500  Cost: 2.58\n",
      "     Total Batches: 11000  Current Batch: 11000  Cost: 2.49\n",
      "     Total Batches: 11500  Current Batch: 11500  Cost: 2.55\n",
      "     Total Batches: 12000  Current Batch: 12000  Cost: 2.53\n",
      "     Total Batches: 12500  Current Batch: 12500  Cost: 2.54\n",
      "     Total Batches: 13000  Current Batch: 13000  Cost: 2.49\n",
      "     Total Batches: 13500  Current Batch: 13500  Cost: 2.40\n",
      "     Total Batches: 14000  Current Batch: 14000  Cost: 2.59\n",
      "     Total Batches: 14500  Current Batch: 14500  Cost: 2.43\n",
      "     Total Batches: 15000  Current Batch: 15000  Cost: 2.50\n",
      "     Total Batches: 15500  Current Batch: 15500  Cost: 2.66\n",
      "     Total Batches: 16000  Current Batch: 16000  Cost: 2.60\n",
      "     Total Batches: 16500  Current Batch: 16500  Cost: 2.58\n",
      "     Total Batches: 17000  Current Batch: 17000  Cost: 2.56\n",
      "     Total Batches: 17500  Current Batch: 17500  Cost: 2.44\n",
      "     Total Batches: 18000  Current Batch: 18000  Cost: 2.61\n",
      "     Total Batches: 18500  Current Batch: 18500  Cost: 2.35\n",
      "     Total Batches: 19000  Current Batch: 19000  Cost: 2.74\n",
      "     Total Batches: 19500  Current Batch: 19500  Cost: 2.56\n",
      "     Total Batches: 20000  Current Batch: 20000  Cost: 2.39\n",
      "     Total Batches: 20500  Current Batch: 20500  Cost: 2.51\n",
      "     Total Batches: 21000  Current Batch: 21000  Cost: 2.48\n",
      "     Total Batches: 21500  Current Batch: 21500  Cost: 2.39\n",
      "     Total Batches: 22000  Current Batch: 22000  Cost: 2.65\n",
      "     Total Batches: 22500  Current Batch: 22500  Cost: 2.66\n",
      "     Total Batches: 23000  Current Batch: 23000  Cost: 2.41\n",
      "     Total Batches: 23500  Current Batch: 23500  Cost: 2.27\n",
      "     Total Batches: 24000  Current Batch: 24000  Cost: 2.42\n",
      "     Total Batches: 24500  Current Batch: 24500  Cost: 2.75\n",
      "     Total Batches: 25000  Current Batch: 25000  Cost: 2.50\n",
      "     Total Batches: 25500  Current Batch: 25500  Cost: 2.57\n",
      "     Total Batches: 26000  Current Batch: 26000  Cost: 2.55\n",
      "     Total Batches: 26500  Current Batch: 26500  Cost: 2.27\n",
      "     Total Batches: 27000  Current Batch: 27000  Cost: 2.47\n",
      "     Total Batches: 27500  Current Batch: 27500  Cost: 2.64\n",
      "     Total Batches: 28000  Current Batch: 28000  Cost: 2.53\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size: %d' % vocabulary_size)\n",
    "\n",
    "# Initiate graph\n",
    "if rnn_flg == 1:\n",
    "    # Use SRN\n",
    "    graph = srn_graph(num_gpus, hidden_size, vocabulary_size, num_unfoldings, \n",
    "                      optimization_frequency, batch_size)\n",
    "elif rnn_flg == 2:\n",
    "    # Use LSTM\n",
    "    graph = lstm_graph(num_gpus, hidden_size, vocabulary_size, num_unfoldings, \n",
    "                       optimization_frequency, batch_size)\n",
    "elif rnn_flg == 3:\n",
    "    # Use SCRN\n",
    "    graph = scrn_graph(num_gpus, alpha, hidden_size, state_size, vocabulary_size, num_unfoldings, \n",
    "                       optimization_frequency, batch_size)\n",
    "    \n",
    "# Optimize graph\n",
    "graph.optimization(learning_rate, learning_decay, num_epochs, summary_frequency, training_text, validation_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
